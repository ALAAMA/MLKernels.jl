\documentclass[12pt, a4paper]{article}
\usepackage{amsfonts, amsmath, amssymb, amsthm} % Mathematics
\usepackage{fullpage, bm}

\DeclareMathOperator{\lag}{lag}
\DeclareMathOperator{\diag}{diag}


\begin{document} % The start of the document

\title{Machine Learning Kernels in Julia}
\author{trthatcher}
\maketitle

Let $k:\mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$ be a kernel function. There are several classes that can be identified. This focusses on kernels of the form:
\begin{equation*}
    k(\mathbf{x}, \mathbf{y}) = \kappa(z(\mathbf{x}, \mathbf{y}))
\end{equation*}
where $z:\mathbb{R}^d \times \mathbb{R}^d \mapsto \mathbb{R}$ and $\kappa:\mathbb{R} \mapsto \mathbb{R}$.

For the first derivatives:
\begin{equation*}
    \frac{\partial \kappa}{\partial \mathbf{x}} 
    = \kappa'(z) \left[\frac{\partial z}{\partial \mathbf{x}}\right]
    \quad \text{and} \quad
    \frac{\partial \kappa}{\partial \mathbf{y}} 
    = \kappa'(z) \left[\frac{\partial z}{\partial \mathbf{y}}\right]
\end{equation*}
Then, the mixed second derivatives are:
\begin{align*}
    \frac{\partial \kappa^2}{\partial \mathbf{x} \partial \mathbf{y}}
    &= \frac{\partial}{\partial \mathbf{x}}\left(\left[\frac{\partial \kappa}{\partial \mathbf{y}}\right]\right)\\
    &= \frac{\partial}{\partial \mathbf{x}}\left(\kappa'(z)
    \left[\frac{\partial z}{\partial \mathbf{y}}\right]\right)\\
    &= \left[\frac{\partial z}{\partial \mathbf{y}}\right]
    \left[\frac{\partial \kappa'(z)}{\partial \mathbf{x}}\right]^{\intercal}
    + \kappa'(z)\frac{\partial}{\partial \mathbf{x}}\left(\left[\frac{\partial z}{\partial \mathbf{y}}\right]\right)\\
    &= \kappa''(z)\left[\frac{\partial z}{\partial \mathbf{y}}\right]
    \left[\frac{\partial z}{\partial \mathbf{x}}\right]^{\intercal}
    + \kappa'(z)\left[\frac{\partial z^2}{\partial \mathbf{x} \partial \mathbf{y}}\right]\\
\end{align*}


\section{Scalar Product Kernel: $z(\mathbf{x}, \mathbf{y}) = \mathbf{x}^\intercal\mathbf{y}$}
The Scalar Product kernel is of the form:
\begin{equation*}
    k(x,y) = \kappa\left(x^\intercal y\right)
\end{equation*}
where $\kappa$ is a function unique to each Scalar Product kernel. For example, the the Sigmoid kernel instance of $\kappa$ is defined to be:
\begin{equation*}
    \kappa_{\mathrm{s}}(z;a,c) = \tanh(az + c)
\end{equation*}

Let $X$ and $Y$ be data matrices with rows as data points. Then the kernel matrix $K$:

\begin{equation*}
    X = \left[
        \begin{array}{c}
            \mathbf{x}_1^{\intercal} \\
            \mathbf{x}_2^{\intercal} \\
            \vdots \\
            \mathbf{x}_n^{\intercal}
        \end{array} 
    \right]
    \text{ and }
    Y = \left[
        \begin{array}{c}
            \mathbf{y}_1^{\intercal} \\
            \mathbf{y}_2^{\intercal} \\
            \vdots \\
            \mathbf{y}_m^{\intercal}
        \end{array} 
    \right]
    \quad \implies \quad
    K = \left[
        \begin{array}{ccc}
            \kappa(\mathbf{x}_1^{\intercal} \mathbf{y}_1) & \cdots & \kappa(\mathbf{x}_1^{\intercal} \mathbf{y}_m) \\
            \vdots & \ddots & \vdots \\
            \kappa(\mathbf{x}_n^{\intercal} \mathbf{y}_1) & \cdots & \kappa(\mathbf{x}_n^{\intercal} \mathbf{y}_m)
        \end{array} 
    \right]
\end{equation*}

Given a set of vectors $S = \{\mathbf{z}_1, ..., \mathbf{z}_n \}$ the gramian is defined as $[\mathbf{G}]_{ij} = \mathbf{z}_i^\intercal \mathbf{z}_j$. Therefore, if we have two sets of vectors $S_x$ and $S_y$ with corresponding data matrices $X$ and $Y$, then we can define the gramian between them as $\mathbf{G}(\mathbf{X}, \mathbf{Y}) = \mathbf{X}\mathbf{Y}^\intercal$. This is really the upper right-hand corner of the gramian of the set $S_x \cup S_y$. 

Finally, we may define the kernel matrix as a function of the gramian:

\begin{equation*}
    [K]_{ij} = \kappa\left([G_{XY}]_{ij}\right)
\end{equation*}

Generally, the best approach to take for implementation is:

\begin{enumerate}
    \item Calculate $G_{XY}$ for matrix $X$ and $Y$ using GEMM or SYRK using BLAS (exploit symmetry of kernels). The option to use rows or columns as data points is left as an option (using trans = 'N' assumes rows are observations and 'T' assumes columns). This is $O(n^3)$ but is heavily optmized to reduce the coefficient.
    \item Scan through the elements of $\mathbf{G}(\mathbf{X},\mathbf{Y})$ and apply the $\kappa$ function to each element, overwrite $G_{XY}$ in the process. This is $O(n^2)$.
    \item Return K
\end{enumerate}

\section{Squared Distance Kernel}
The Squared Distance kernel is of the form:
\begin{equation*}
    k(x,y) = \kappa\left((||x - y||^2\right)
\end{equation*}
where $\kappa$ is a function unique to each kernel.  For example, the the Gaussian kernel instance of $\kappa$ is defined to be:
\begin{equation*}
    \kappa_{\Phi}(z;\sigma) = \exp\left(\frac{-z}{2\sigma^2}\right)
\end{equation*}

\subsection{Kernel Matrix}
Let $\mathbf{X}$ and $\mathbf{Y}$ be data matrices with rows as data points. Then the kernel matrix $K$:
\begin{equation*}
    \mathbf{X} = \left[
        \begin{array}{c}
            \mathbf{x}_1^{\intercal} \\
            \mathbf{x}_2^{\intercal} \\
            \vdots \\
            \mathbf{x}_n^{\intercal}
        \end{array} 
    \right]
    \text{ and }
    \mathbf{Y} = \left[
        \begin{array}{c}
            \mathbf{y}_1^{\intercal} \\
            \mathbf{y}_2^{\intercal} \\
            \vdots \\
            \mathbf{y}_m^{\intercal}
        \end{array} 
    \right]
    \quad \implies \quad
    \mathbf{K} = \left[
        \begin{array}{ccc}
            \kappa\left(||\mathbf{x}_1 - \mathbf{y}_1||^2\right) & \cdots & \kappa\left(||\mathbf{x}_1 - \mathbf{y}_m||^2\right) \\
            \vdots & \ddots & \vdots \\
            \kappa\left(||\mathbf{x}_n - \mathbf{y}_1||^2\right) & \cdots & \kappa\left(||\mathbf{x}_n - \mathbf{y}_m||^2\right)
        \end{array} 
    \right]
\end{equation*}
For a vector $\mathbf{x}$ and $\mathbf{y}$, define the $\lag$ (alternatively, the error) between the two vectors to be:
\begin{equation*}
    \lag(x,y) = x - y = \epsilon
\end{equation*}
Then we may define kernels of this form as:
\begin{equation*}
    k(x,y) = \kappa\left(\lag(\mathbf{x},\mathbf{y})\cdot \lag(\mathbf{x}, \mathbf{y})\right)
           = \kappa\left(\epsilon^\intercal\epsilon\right)
           = \kappa\left(||\mathbf{x}-\mathbf{y}||^2\right)
\end{equation*}
Given a set of vectors $S = \{\mathbf{z}_1, ..., \mathbf{z}_n \}$, the lagged gramian is defined as:
\begin{equation*}
    [\mathbf{G}_l]_{ij} = ||\mathbf{z}_i - \mathbf{z}_j||^2
\end{equation*}
Similarly, for two data matrices $\mathbf{X}$ and $\mathbf{Y}$:
\begin{equation*}
    \left[\mathbf{G}_{l}\left(\mathbf{X}, \mathbf{Y}\right)\right]_{ij} = ||\mathbf{x}_i - \mathbf{y}_j||^2
\end{equation*}
Where $\mathbf{x}_i$ and $\mathbf{y}_j$ are the data vectors in $\mathbf{X}$ and $\mathbf{Y}$. The lagged gramian can be defined in terms of $\mathbf{G}(\mathbf{X}, \mathbf{Y})$, $\mathbf{X}$ and $\mathbf{Y}$.
\begin{equation*}
    \left[\mathbf{G}_{l}\left(\mathbf{X}, \mathbf{Y}\right)\right]_{ij} 
    = \left[\diag\left(\mathbf{G}(\mathbf{X})\right)\right]_i - 2\left[\mathbf{G}\right]_{ij} 
    + \left[\diag\left(\mathbf{G}(\mathbf{Y})\right)\right]_j
\end{equation*}
The function $\diag\left(\mathbf{G}(\mathbf{X})\right)$ returns a vector of diagonal entries of $\mathbf{G}$. This is simply a vector where entry $i$ is $\mathbf{x}_i^{\intercal}\mathbf{x}_i$. 

\subsection{Kernel Matrix Calculation - Implementation}
Therefore, to compute the kernel matrix for a squared distance kernel:
\begin{enumerate}
    \item Calculate $\mathbf{G}(\mathbf{X},\mathbf{Y})$ for matrix $\mathbf{X}$ and $\mathbf{Y}$ using GEMM or SYRK using BLAS (exploit symmetry of kernels). This is $O(n^3)$.
    \item Transform $\mathbf{G}(\mathbf{X},\mathbf{Y})$ to $\mathbf{G}_l(\mathbf{X},\mathbf{Y})$. This is $O(n^2)$.
    \item Scan through the elements of $\mathbf{G}_l(\mathbf{X},\mathbf{Y})$ and apply the $\kappa$ function to each element, overwrite $\mathbf{G}_l(\mathbf{X},\mathbf{Y})$ in the process. This is $O(n^2)$.
    \item Return K
\end{enumerate}

\subsection{Kernel Derivative}
\begin{equation*}
    \frac{\partial z}{\partial \mathbf{x}}
    = \frac{\partial ||\mathbf{x}-\mathbf{y}||^2}{\partial \mathbf{x}}
    = \frac{\partial}{\partial \mathbf{x}} \left(\mathbf{x}^\intercal\mathbf{x} - 2\mathbf{x}^{\intercal}\mathbf{y} + \mathbf{y}^\intercal\mathbf{y}\right)
    =2 (\mathbf{x} - \mathbf{y})
\end{equation*}
\begin{equation*}
    \frac{\partial z}{\partial \mathbf{y}}
    = \frac{\partial ||\mathbf{x}-\mathbf{y}||^2}{\partial \mathbf{y}}
    = \frac{\partial}{\partial \mathbf{y}} \left(\mathbf{x}^\intercal\mathbf{x} - 2\mathbf{x}^{\intercal}\mathbf{y} + \mathbf{y}^\intercal\mathbf{y}\right)
    =2 (\mathbf{y} - \mathbf{x})
\end{equation*}
Using the identity:
\begin{equation*}
    \left[\frac{\partial \mathbf{u}}{\partial \mathbf{v}}\right]_{ij}
    = \frac{\partial u_i}{\partial v_i}
\end{equation*}
The second mixed derivative is:
\begin{equation*}
    \frac{\partial z^2}{\partial \mathbf{x} \partial\mathbf{y}}
    = \frac{\partial ||\mathbf{x}-\mathbf{y}||^2}{\partial \mathbf{x} \partial\mathbf{y}}
    = \frac{\partial}{\partial \mathbf{x}}  2 (\mathbf{y} - \mathbf{x})
    = -2\mathbf{I}_d
\end{equation*}
Substituting in the above into the formula for kernel derivative:
\begin{align*}
    \frac{\partial \kappa^2}{\partial \mathbf{x} \partial \mathbf{y}}
    &= \kappa''(z)\left[\frac{\partial z}{\partial \mathbf{y}}\right]
    \left[\frac{\partial z}{\partial \mathbf{x}}\right]^{\intercal}
    + \kappa'(z)\left[\frac{\partial z^2}{\partial \mathbf{x} \partial \mathbf{y}}\right]\\
    &= -4\kappa''(z)(\mathbf{x} - \mathbf{y})(\mathbf{x} - \mathbf{y})^{\intercal}
    - 2 \kappa'(z)\mathbf{I}_d
\end{align*}
Define $\bm{\epsilon} = \mathbf{x} - \mathbf{y}$, then this gives an element-wise formula of:
\begin{equation*}
    \left[\frac{\partial \kappa^2}{\partial \mathbf{x} \partial \mathbf{y}}\right]_{ij}
    = \begin{cases}
        -4\kappa''(\bm{\epsilon}^\intercal\bm{\epsilon})\epsilon_i \epsilon_j & \text{if } i \neq j\\
        -4\kappa''(\bm{\epsilon}^\intercal\bm{\epsilon})\epsilon_i^2 - 2\kappa'(\bm{\epsilon}^\intercal\bm{\epsilon}) & \text{if } i = j
    \end{cases}
\end{equation*}


\end{document}
